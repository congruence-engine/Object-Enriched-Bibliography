{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "This notebook outlines an experimental pipeline for matching museum objects with academic research articles.\n",
        "\n",
        "It was developed as part of the 'Building an Object-Enriched Bibliography: Experiments in Linking Museum Objects and Academic Literature' investigation during the [Congruence Engine](https://perma.cc/58MF-4XWV) project at the Science Museum.\n",
        "\n",
        "In this experiment, we were interested in finding ways of linking academic literature with musseum collections with relevance to the textile industry. We have therefore chosen three open-access academic articles that discuss textiles, as well as a sample dataset of textiles-related collections items from the Science Museum Group's [Collections Online](https://collection.sciencemuseumgroup.org.uk/).\n",
        "\n",
        "#### Pipeline summary\n",
        "\n",
        "\n",
        "*   Acquire full text articles from the [CORE API](https://api.core.ac.uk/docs/v3)\n",
        "*   Load Science Museum Group collections data\n",
        "*   Identify entities in one article using the named entity recognition (NER) model [GLiNER](https://github.com/urchade/GLiNER)\n",
        "*   Create embeddings for entities and museum objects using [Sentence Transformers](https://sbert.net/)\n",
        "*   Visualise entities and objects in vector space\n",
        "\n",
        "Parts of this code were written with the help of Chat GPT 4o.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pxzDptXbhaJh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install the required packages"
      ],
      "metadata": {
        "id": "tZxDGsv6Vf4M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install umap-learn gliner"
      ],
      "metadata": {
        "id": "LZG-Yz8AjN6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import packages"
      ],
      "metadata": {
        "id": "Z7CcpJimVqNp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from gliner import GLiNER\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import requests\n",
        "import json\n",
        "import plotly.express as px\n",
        "from umap.umap_ import UMAP"
      ],
      "metadata": {
        "id": "o22UlftRSON5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Acquire Full Text Articles\n",
        "For this example we are going to use the CORE API, 'The worldâ€™s largest collection of open access research papers'. You can read the [API documentation here](https://api.core.ac.uk/docs/v3#section/Welcome!).\n",
        "\n",
        "We will pull just three articles from the API in this case, but this could be performed at a much larger scale if desired. To make things easier, we have pre-selected the identifiers for our three articles in question.\n",
        "\n",
        "Although not strictly necessary, we recommend that you use a free [API Key](https://core.ac.uk/services/api#what-is-included). The code below assumes that you have a acquired an API key first."
      ],
      "metadata": {
        "id": "hhHTli3WQQht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = \"YOUR_API_KEY\" # Replace with your API key\n",
        "\n",
        "identifiers = [\"52191\", \"7314055\", \"573860020\"] # These are the identifiers for the three articles that we have selected\n",
        "                                                # Feel free to add more!\n",
        "\n",
        "url_template = \"https://api.core.ac.uk/v3/outputs/{identifier}\"\n",
        "\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {api_key}\",\n",
        "}\n",
        "\n",
        "data = []\n",
        "\n",
        "for identifier in identifiers:\n",
        "    url = url_template.format(identifier=identifier)\n",
        "    response = requests.get(url, headers=headers)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        article_data = response.json()\n",
        "        data.append(article_data)\n",
        "    else:\n",
        "        print(f\"Failed to fetch article {identifier}: {response.status_code} {response.reason}\")\n",
        "\n",
        "# Save the collected data to a JSON file\n",
        "output_file = \"/content/articles_data.json\" # This will save in your Colab environment. You may wish to change the directory\n",
        "with open(output_file, \"w\") as file:\n",
        "    json.dump(data, file, indent=4)\n",
        "\n",
        "print(f\"Data for {len(data)} articles saved to {output_file}\")"
      ],
      "metadata": {
        "id": "ddufde-eQ3WX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load objects and articles data\n",
        "\n"
      ],
      "metadata": {
        "id": "LJnOx6jpVxLx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we will load the JSON file that we have retrieved from the CORE database"
      ],
      "metadata": {
        "id": "jzWBHC4JmX6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "articles_df = pd.read_json('/content/articles_data.json')\n"
      ],
      "metadata": {
        "id": "qV-2ObIFRlaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "articles_df.head()"
      ],
      "metadata": {
        "id": "He2iVjnMRuom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can load our museum objects data. While you could retrieve this from the [Science Museum Group API](https://www.sciencemuseumgroup.org.uk/our-work/our-collection/using-our-collection-api), we have sped things up by adding a csv file to GitHub. Bear in mind that this data dates from August 2024, and the most up-to-date information will always be accessible via the API."
      ],
      "metadata": {
        "id": "nKxtf-hLmg4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "objects_df = pd.read_csv('https://raw.githubusercontent.com/congruence-engine/Object-Enriched-Bibliography/refs/heads/main/datasets/objects_metadata.csv')"
      ],
      "metadata": {
        "id": "F-GwrwDiluVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "objects_df.head()"
      ],
      "metadata": {
        "id": "MCGDLLOISHqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "object_texts = objects_df['Collection Online Title'].tolist()\n"
      ],
      "metadata": {
        "id": "4AEAzzfDSVAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Identify 'object' entities using GLiNER\n",
        "We do this using GLiNER, a 'universal' named entity recognition model.\n",
        "Here we have used **[gliner_medium-v2.1](https://huggingface.co/urchade/gliner_medium-v2.1)**.\n",
        "\n",
        "In this instance we extract entities that the model recognises as 'objects' in a single article. You can run this separately on each of the three articles that we pulled from the CORE database if you like, to see how the extracted entities differ.\n",
        "\n",
        "To see a full list of models in the GLiNER family, visit the model's [github repository](https://github.com/urchade/GLiNER)."
      ],
      "metadata": {
        "id": "l86j1-DMV5GJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = GLiNER.from_pretrained(\"urchade/gliner_medium-v2.1\")"
      ],
      "metadata": {
        "id": "KtZYbsjzSddb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the article text. In this case, we'll use index 0 which is the first article.\n",
        "# You can change this number to 1 or 2 to test the other articles later on.\n",
        "text = articles_df['fullText'].iloc[0]\n",
        "\n",
        "# The code below will chunk the text so that it does not exceed the maximum content length of the model (which is 384 tokens)\n",
        "def chunk_text(text, max_words=200):\n",
        "    words = text.split()\n",
        "    chunks = [\" \".join(words[i:i + max_words]) for i in range(0, len(words), max_words)]\n",
        "    return chunks\n",
        "\n",
        "chunks = chunk_text(text, max_words=120)"
      ],
      "metadata": {
        "id": "aXb7JP-YSkwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = [\"object\"] # This defines the entities that we will be looking for in the text.\n",
        "                    # Here we have gone for a very vague 'objects' category, but you can be as specific as you like\n",
        "                    # You can also include multiple labels for different entities\n",
        "\n",
        "all_entities = []\n",
        "\n",
        "# Run GLiNER on each chunk and collect the entities\n",
        "for chunk in chunks:\n",
        "    entities = model.predict_entities(chunk, labels, threshold=0.4)\n",
        "    all_entities.extend(entities)"
      ],
      "metadata": {
        "id": "RmT1-U3DSmpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract unique object terms from the entities\n",
        "article_objects = list(set(entity['text'] for entity in all_entities if entity['label'] == \"object\"))\n",
        "print(\"Identified Objects:\", article_objects)"
      ],
      "metadata": {
        "id": "2oKDqxQWSt6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(article_objects)"
      ],
      "metadata": {
        "id": "Bp-LpKZ6SzKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create embeddings\n",
        "This section of code will create embeddings for:\n",
        "\n",
        "*   the 'object' entities extracted using GLiNER\n",
        "*   the full dataset of objects from a subset of 'textile' objects in the SMG collection\n",
        "\n",
        "To generate these embeddings, we will be using the Sentence Transformer **all-MiniLM-L6-v2**.\n",
        "\n"
      ],
      "metadata": {
        "id": "d6zav54nWqVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize embedding model\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')"
      ],
      "metadata": {
        "id": "T18zUC9LUewM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate embeddings for each identified 'object'' entity\n",
        "article_embeddings = {term: embedding_model.encode(term) for term in article_objects}\n",
        "\n",
        "# Encode museum object descriptions with the same model\n",
        "object_texts = objects_df['Collection Online Title'].tolist()\n",
        "objects_embeddings = embedding_model.encode(object_texts)\n",
        "\n",
        "# Find closest matches between textile machine terms and museum objects\n",
        "matches = []\n",
        "for term, term_embedding in article_embeddings.items():\n",
        "    similarities = cosine_similarity([term_embedding], objects_embeddings)[0]\n",
        "    best_match_idx = np.argmax(similarities)\n",
        "    best_match_score = similarities[best_match_idx]\n",
        "\n",
        "    matches.append({\n",
        "        'article_object': term,\n",
        "        'best_match_object': objects_df['Collection Online Title'].iloc[best_match_idx],\n",
        "        'similarity_score': best_match_score\n",
        "    })\n",
        "\n",
        "# Convert matches to DataFrame for easy viewing\n",
        "matches_df = pd.DataFrame(matches)\n",
        "matches_df = matches_df.sort_values(by='similarity_score', ascending=False)"
      ],
      "metadata": {
        "id": "tsZZ0dKXfubf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matches_df"
      ],
      "metadata": {
        "id": "2HQbscnJf-QR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualise the results\n",
        "\n",
        "Here we visualise the results in vector space. This will help us to see how closely the extracted entities match the objects from the SMG dataset.\n",
        "\n",
        "You can try the full process with each of the three articles that we loaded originally from CORE. You'll notice that one of the articles returns many more candidates than the others!"
      ],
      "metadata": {
        "id": "RrHZjAS6VDuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine textile machine and object embeddings\n",
        "combined_embeddings = np.vstack((list(article_embeddings.values()), objects_embeddings))\n",
        "\n",
        "# Create labels for visualization\n",
        "labels = ['NER Object'] * len(article_embeddings) + ['SMG Object'] * len(objects_embeddings)"
      ],
      "metadata": {
        "id": "5OHkamCJUj8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "reducer = UMAP(n_components=3, n_neighbors=15, min_dist=0.1, metric='cosine', random_state=42)\n",
        "embedding_3d = reducer.fit_transform(combined_embeddings)"
      ],
      "metadata": {
        "id": "PfthlablUn7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "object_name = list(article_objects) + objects_df['Collection Online Title'].tolist()\n"
      ],
      "metadata": {
        "id": "GP3Zs8bRc2ZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_df = pd.DataFrame({\n",
        "    'x': embedding_3d[:, 0],\n",
        "    'y': embedding_3d[:, 1],\n",
        "    'z': embedding_3d[:, 2],\n",
        "    'label': labels,\n",
        "    'name': object_name\n",
        "})\n",
        "\n",
        "fig = px.scatter_3d(\n",
        "    embedding_df,\n",
        "    x='x',\n",
        "    y='y',\n",
        "    z='z',\n",
        "    color='label',\n",
        "    hover_data={\n",
        "        'x': False,\n",
        "        'y': False,\n",
        "        'z': False,\n",
        "        'label': True,\n",
        "        'name': True\n",
        "    },\n",
        "    title='NER Terms and Museum Objects',\n",
        "    opacity=0.7\n",
        ")\n",
        "\n",
        "fig.update_traces(marker=dict(size=4))\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "DXOM7FfvU7ZA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}